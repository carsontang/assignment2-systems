{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS336 Assignment 2: Systems - Google Colab Setup\n",
    "\n",
    "This notebook sets up and runs CS336 Assignment 2 in Google Colab with GPU support.\n",
    "\n",
    "**Important**: Make sure to enable GPU in Colab:\n",
    "- Go to Runtime → Change runtime type → Hardware accelerator → GPU (T4 or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU and Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Clone the repository (replace with your repo URL)\n",
    "# Option 1: Public repo\n",
    "!git clone https://github.com/YOUR_USERNAME/assignment2-systems.git\n",
    "\n",
    "# Option 2: Private repo (you'll need to authenticate)\n",
    "# !git clone https://YOUR_TOKEN@github.com/YOUR_USERNAME/assignment2-systems.git\n",
    "\n",
    "# Option 3: Upload a zip file\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # Upload cs336_assignment2_cloud.tar.gz\n",
    "# !tar -xzf cs336_assignment2_cloud.tar.gz\n",
    "\n",
    "%cd assignment2-systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (Colab usually has it pre-installed, but let's ensure CUDA version)\n",
    "import torch\n",
    "print(f\"Existing PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install -q numpy tqdm matplotlib pandas pytest regex humanfriendly wandb triton\n",
    "\n",
    "# Install local packages\n",
    "!pip install -e ./cs336-basics\n",
    "!pip install -e .\n",
    "\n",
    "# Verify installation\n",
    "import cs336_basics\n",
    "import cs336_systems\n",
    "print(\"\\nModules imported successfully!\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Benchmarking Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with default parameters\n",
    "!python cs336_systems/benchmarking_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with custom parameters for larger model\n",
    "!python cs336_systems/benchmarking_script.py \\\n",
    "    --batchsize 64 \\\n",
    "    --context 256 \\\n",
    "    --dmodel 1024 \\\n",
    "    --nlayers 24 \\\n",
    "    --nheads 16 \\\n",
    "    --dff 4096 \\\n",
    "    --num-runs 5 \\\n",
    "    --num-warmup 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Interactive Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/assignment2-systems')\n",
    "\n",
    "from cs336_systems.benchmarking_script import Config, benchmark_model, generate_random_data\n",
    "from cs336_basics.model import BasicsTransformerLM\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test different model sizes\n",
    "results = []\n",
    "model_sizes = [\n",
    "    {\"name\": \"Small\", \"d_model\": 256, \"num_layers\": 6, \"num_heads\": 8},\n",
    "    {\"name\": \"Medium\", \"d_model\": 512, \"num_layers\": 12, \"num_heads\": 8},\n",
    "    {\"name\": \"Large\", \"d_model\": 768, \"num_layers\": 12, \"num_heads\": 12},\n",
    "]\n",
    "\n",
    "for size in model_sizes:\n",
    "    config = Config(\n",
    "        batch_size=32,\n",
    "        vocab_size=10000,\n",
    "        context_length=128,\n",
    "        d_model=size[\"d_model\"],\n",
    "        num_layers=size[\"num_layers\"],\n",
    "        num_heads=size[\"num_heads\"],\n",
    "        d_ff=size[\"d_model\"] * 4\n",
    "    )\n",
    "    \n",
    "    model = BasicsTransformerLM(\n",
    "        vocab_size=config.vocab_size,\n",
    "        context_length=config.context_length,\n",
    "        d_model=config.d_model,\n",
    "        num_layers=config.num_layers,\n",
    "        num_heads=config.num_heads,\n",
    "        d_ff=config.d_ff,\n",
    "        rope_theta=config.rope_theta\n",
    "    ).cuda()\n",
    "    \n",
    "    x = generate_random_data(config).cuda()\n",
    "    \n",
    "    print(f\"\\nBenchmarking {size['name']} model...\")\n",
    "    timing = benchmark_model(model, x, num_runs=5, num_warmup=2)\n",
    "    \n",
    "    results.append({\n",
    "        \"name\": size[\"name\"],\n",
    "        \"params\": sum(p.numel() for p in model.parameters()),\n",
    "        \"avg_time\": timing[\"avg\"],\n",
    "        \"min_time\": timing[\"min\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Parameters: {results[-1]['params']:,}\")\n",
    "    print(f\"Avg time: {timing['avg']:.4f}s\")\n",
    "\n",
    "# Plot results\n",
    "names = [r[\"name\"] for r in results]\n",
    "times = [r[\"avg_time\"] for r in results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(names, times)\n",
    "plt.xlabel(\"Model Size\")\n",
    "plt.ylabel(\"Average Forward Pass Time (s)\")\n",
    "plt.title(\"Model Size vs Inference Time\")\n",
    "for i, (name, time) in enumerate(zip(names, times)):\n",
    "    plt.text(i, time + 0.001, f\"{time:.4f}s\", ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests\n",
    "!pytest -v ./tests/\n",
    "\n",
    "# Or run specific test files\n",
    "# !pytest -v ./tests/test_attention.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Memory Profiling (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile GPU memory usage\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def get_gpu_memory():\n",
    "    return torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
    "\n",
    "# Clear GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Initial GPU memory: {get_gpu_memory():.2f} GB\")\n",
    "\n",
    "# Create a large model\n",
    "config = Config(\n",
    "    batch_size=32,\n",
    "    vocab_size=50000,\n",
    "    context_length=512,\n",
    "    d_model=1024,\n",
    "    num_layers=24,\n",
    "    num_heads=16,\n",
    "    d_ff=4096\n",
    ")\n",
    "\n",
    "model = BasicsTransformerLM(\n",
    "    vocab_size=config.vocab_size,\n",
    "    context_length=config.context_length,\n",
    "    d_model=config.d_model,\n",
    "    num_layers=config.num_layers,\n",
    "    num_heads=config.num_heads,\n",
    "    d_ff=config.d_ff,\n",
    "    rope_theta=config.rope_theta\n",
    ").cuda()\n",
    "\n",
    "print(f\"After model creation: {get_gpu_memory():.2f} GB\")\n",
    "\n",
    "# Run forward pass\n",
    "x = generate_random_data(config).cuda()\n",
    "with torch.no_grad():\n",
    "    output = model(x)\n",
    "\n",
    "print(f\"After forward pass: {get_gpu_memory():.2f} GB\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}